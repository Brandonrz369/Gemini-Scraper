**User:**

<task>
Scan my code abse
</task>
<environment_details>
# VSCode Visible Files
frontend/script.js

# VSCode Open Tabs
frontend/index.html
backend/modules/request_handler.py
backend/modules/content_analyzer.py
../../Documents/Cline/MCP/perplexity-mcp/test_mcp.js
.roo/mcp.json
../../.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json
backend/modules/ai_handler.py
backend/config/settings.py
backend/main.py
frontend/script.js
backend/modules/output_generator.py

# Current Time
4/7/2025, 12:53:04 AM (UTC, UTC+0:00)

# Current Working Directory (/home/brandonruizmarketing/Desktop/Gemini-Scraper) Files
cline_task_apr-6-2025_5-38-40-am.md
new york web_html_info design jobs - craigslist.html
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist.html
Web_Html_Info Design Jobs near Croton, OH - craigslist.html
backend/
backend/main.py
backend/requirements.txt
backend/run.log
backend/config/
backend/config/__init__.py
backend/config/settings.py
backend/data/
backend/data/cities_large.json
backend/data/cities_medium.json
backend/data/cities_small.json
backend/data/daily_leads_2025-04-01.csv
backend/data/daily_leads_2025-04-06.csv
backend/data/leads_dashboard.html
backend/data/leads.db
backend/modules/
backend/modules/__init__.py
backend/modules/ai_handler.py
backend/modules/city_manager.py
backend/modules/content_analyzer.py
backend/modules/output_generator.py
backend/modules/request_handler.py
backend/modules/state_manager.py
frontend/
frontend/index.html
frontend/script.js
frontend/style.css
frontend/public/
frontend/public/graded_leads.json
new york web_html_info design jobs - craigslist_files/
new york web_html_info design jobs - craigslist_files/00N0N_747nXvHEtIp_0as05U_50x50c.jpg
new york web_html_info design jobs - craigslist_files/59219857f2acaf38f9f5507d73c66d097cefa075.js
new york web_html_info design jobs - craigslist_files/localStorage-092e9f9e2f09450529e744902aa7cdb3a5cc868d.html
new york web_html_info design jobs - craigslist_files/manifest.js
new york web_html_info design jobs - craigslist_files/search-2025-03-31-14-03-5bdeda4af7be13ab95567dbefab8f0ddf69f1.js
new york web_html_info design jobs - craigslist_files/www-2025-03-31-14-03-9ae1ece27ab3f4d31d0a9003954f15e2d207e51.css
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/00N0N_747nXvHEtIp_0as05U_600x450.jpg
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/6152_002.png
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/6152.png
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/6153_002.png
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/6153.png
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/59219857f2acaf38f9f5507d73c66d097cefa075.js
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/browsePostings-2025-03-14-07-08-f6311623383c04c253b32602edbf2.js
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/localStorage-092e9f9e2f09450529e744902aa7cdb3a5cc868d.html
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/manifest.js
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/marker-icon.png
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/marker-shadow.png
SoundCloud Web Developer - web _ html _ info design - job employment - craigslist_files/posting-2025-03-31-14-03-f7cafa949c753004c3ac16b7ae1c29a7bdf.css
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/00E0E_7nJSLSVFdVI_0CI0t2_50x50c.jpg
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/00h0h_Avo3LZOcMR_0wU0oM_50x50c.jpg
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/00i0i_8QdGRj8iucO_06s06O_50x50c_002.jpg
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/00i0i_8QdGRj8iucO_06s06O_50x50c.jpg
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/00N0N_747nXvHEtIp_0as05U_50x50c.jpg
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/00s0s_96EdIwm1cZL_0jm0t2_50x50c.jpg
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/00606_b73PnXFj43S_0gw0gw_50x50c.jpg
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/00707_aMIwPQSWPd5_0dL07K_50x50c.jpg
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/59219857f2acaf38f9f5507d73c66d097cefa075.js
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/localStorage-092e9f9e2f09450529e744902aa7cdb3a5cc868d.html
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/manifest.js
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/search-2025-03-31-14-03-5bdeda4af7be13ab95567dbefab8f0ddf69f1.js
Web_Html_Info Design Jobs near Croton, OH - craigslist_files/www-2025-03-31-14-03-9ae1ece27ab3f4d31d0a9003954f15e2d207e51.css

# Current Mode
PLAN MODE
In this mode you should focus on information gathering, asking questions, and architecting a solution. Once you have a plan, use the plan_mode_respond tool to engage in a conversational back and forth with the user. Do not use the plan_mode_respond tool until you've gathered all the information you need e.g. with read_file or ask_followup_question.
(Remember: If it seems the user wants you to use tools only available in Act Mode, you should ask the user to "toggle to Act mode" (use those words) - they will have to manually do this themselves with the Plan/Act toggle button below. You do not have the ability to switch to Act Mode yourself, and must wait for the user to do it themselves once they are satisfied with the plan. You also cannot present an option to toggle to Act mode, as this will be something you need to direct the user to do manually themselves.)
</environment_details>

---

**Assistant:**

<plan_mode_respond>
<response>Okay, I can help you scan your codebase located in `/home/brandonruizmarketing/Desktop/Gemini-Scraper`.

To make sure I perform the right kind of scan, could you please clarify what you'd like me to look for? For example, are you interested in:

1.  **Code Structure Overview:** Listing the main functions, classes, and methods in the Python backend and JavaScript frontend?
2.  **Finding Specific Patterns:** Searching for things like TODO comments, specific function calls, or potential areas for refactoring?
3.  **Code Quality/Style:** Checking for adherence to style guides (though I'd need to know which linters/formatters you use, like Black/Flake8 for Python or ESLint/Prettier for JS)?
4.  **Potential Issues:** Looking for common anti-patterns or potential bugs (this would be a more general analysis)?
5.  **Something else?** (Please specify)

Once I understand the goal of the scan, I can propose a plan using the available tools.</response>
<options>
["Code Structure Overview", "Finding Specific Patterns (e.g., TODOs)", "Code Quality/Style (if linters are set up)", "Potential Issues/Anti-patterns", "Something else"]
</options>
</plan_mode_respond>

---

**User:**

[plan_mode_respond] Result:
<user_message>
you now have mpc server access remember where we left off?
</user_message>
<environment_details>
# VSCode Visible Files
frontend/script.js

# VSCode Open Tabs
frontend/index.html
backend/modules/request_handler.py
backend/modules/content_analyzer.py
../../Documents/Cline/MCP/perplexity-mcp/test_mcp.js
.roo/mcp.json
../../.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json
backend/modules/ai_handler.py
backend/config/settings.py
backend/main.py
frontend/script.js
backend/modules/output_generator.py

# Current Time
4/7/2025, 12:54:47 AM (UTC, UTC+0:00)

# Current Mode
PLAN MODE
In this mode you should focus on information gathering, asking questions, and architecting a solution. Once you have a plan, use the plan_mode_respond tool to engage in a conversational back and forth with the user. Do not use the plan_mode_respond tool until you've gathered all the information you need e.g. with read_file or ask_followup_question.
(Remember: If it seems the user wants you to use tools only available in Act Mode, you should ask the user to "toggle to Act mode" (use those words) - they will have to manually do this themselves with the Plan/Act toggle button below. You do not have the ability to switch to Act Mode yourself, and must wait for the user to do it themselves once they are satisfied with the plan. You also cannot present an option to toggle to Act mode, as this will be something you need to direct the user to do manually themselves.)
</environment_details>

---

**Assistant:**

<plan_mode_respond>
<response>Yes, I remember we connected the `researcher-mcp` server.

Regarding scanning your codebase in `/home/brandonruizmarketing/Desktop/Gemini-Scraper`, I previously asked what specific type of scan you'd like me to perform. Could you clarify if you're interested in:

1.  **Code Structure Overview:** Listing functions/classes in the Python backend and JS frontend?
2.  **Finding Specific Patterns:** Searching for TODOs, specific function calls, etc.?
3.  **Code Quality/Style:** Checking against linters/formatters?
4.  **Potential Issues:** Looking for anti-patterns or potential bugs?
5.  **Something else?**

Once I know the goal, I can formulate a plan. Depending on the task, we might even leverage the `researcher-mcp` tools for analysis or suggestions.</response>
<options>
["Code Structure Overview", "Finding Specific Patterns (e.g., TODOs)", "Code Quality/Style (if linters are set up)", "Potential Issues/Anti-patterns", "Something else"]
</options>
</plan_mode_respond>

---

**User:**

[plan_mode_respond] Result:
<user_message>
no our first major discory is how you can search a whole state instead of going city by city by using this url paramiter. https://limaohio.craigslist.org/search/rushsylvania-oh/ggg?lat=40.4389&lon=-83.7035&postal=92338&query=website&search_distance=1000#search=2~thumb~1668 

use perplexity mpc to see whats the best way to tackle this and verify what im saying
</user_message>
<environment_details>
# VSCode Visible Files
frontend/script.js

# VSCode Open Tabs
frontend/index.html
backend/modules/request_handler.py
backend/modules/content_analyzer.py
../../Documents/Cline/MCP/perplexity-mcp/test_mcp.js
.roo/mcp.json
../../.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json
backend/modules/ai_handler.py
backend/config/settings.py
backend/main.py
frontend/script.js
backend/modules/output_generator.py

# Current Time
4/7/2025, 12:58:29 AM (UTC, UTC+0:00)

# Current Mode
PLAN MODE
In this mode you should focus on information gathering, asking questions, and architecting a solution. Once you have a plan, use the plan_mode_respond tool to engage in a conversational back and forth with the user. Do not use the plan_mode_respond tool until you've gathered all the information you need e.g. with read_file or ask_followup_question.
(Remember: If it seems the user wants you to use tools only available in Act Mode, you should ask the user to "toggle to Act mode" (use those words) - they will have to manually do this themselves with the Plan/Act toggle button below. You do not have the ability to switch to Act Mode yourself, and must wait for the user to do it themselves once they are satisfied with the plan. You also cannot present an option to toggle to Act mode, as this will be something you need to direct the user to do manually themselves.)
</environment_details>

---

**Assistant:**

Okay, I understand. You've found a Craigslist URL structure that seems to allow searching within a large radius using latitude, longitude, postal code, and search distance, potentially covering a whole state or large region instead of just one city's subdomain. You want me to use the Perplexity MCP tool to analyze this and figure out the best way to incorporate this into the scraper.

This is an interesting discovery! Let's use the `reason` tool from the `researcher-mcp` to investigate this URL structure and its potential.

```tool_code
<use_mcp_tool>
<server_name>researcher-mcp</server_name>
<tool_name>reason</tool_name>
<arguments>
{
  "query": "Analyze the following Craigslist search URL: https://limaohio.craigslist.org/search/rushsylvania-oh/ggg?lat=40.4389&lon=-83.7035&postal=92338&query=website&search_distance=1000#search=2~thumb~1668. Specifically, focus on the parameters: lat, lon, postal, and search_distance. Does using these parameters, especially search_distance=1000 (miles?), allow searching a very large radius, potentially covering an entire state or region, effectively bypassing the need to iterate through individual city subdomains (like limaohio.craigslist.org)? What are the potential benefits and drawbacks of using this method for scraping Craigslist for leads across a wide geographical area like a state? How reliable is this likely to be, considering Craigslist might change undocumented parameters?"
}
</arguments>
</use_mcp_tool>
```

---

**User:**

[use_mcp_tool for 'researcher-mcp'] Result:
<think>
Okay, let's tackle this. The user wants an analysis of a specific Craigslist URL, focusing on the parameters lat, lon, postal, search_distance, and how using these might allow scraping a large area without city subdomains.

First, I need to parse the URL provided: https://limaohio.craigslist.org/search/rushsylvania-oh/ggg?lat=40.4389&lon=-83.7035&postal=92338&query=website&search_distance=1000#search=2~thumb~1668. The main parameters here are lat, lon, postal, search_distance. The user is asking if search_distance=1000 (miles?) lets you cover a big area, maybe a whole state, thus avoiding the need to check each city subdomain.

Looking at the search results, source [1] mentions that Craigslist cities are subdomains, like raleigh.craigslist.org. But if you can set a large search_distance, maybe you don't need to iterate through each city. Source [2] talks about narrowing by location using zip and miles, which aligns with the parameters here. Source [3] and [4] discuss scraping methods, including using APIs and parameters. Source [4] has code examples using filters like max_price and search_distance. Source [6] is about a Python wrapper that might use these parameters.

The key is to determine what search_distance=1000 means. Is it miles or kilometers? Craigslist typically uses miles in the US. 1000 miles from a central point could cover a large region, possibly multiple states. For example, from Lima, OH, 1000 miles would reach far beyond Ohio. But does Craigslist allow such a large radius? Maybe, but there might be limits. The postal code 92338 is actually a California ZIP, but the subdomain is limaohio, which is in Ohio. That discrepancy might affect results.

Benefits of using this method: Avoid scraping each subdomain, fewer API calls, centralized data collection. Drawbacks: Possible inaccuracies due to postal vs. subdomain mismatch, Craigslist might limit max search_distance, or parameters could change. Also, overlapping results or missed listings if the search isn't comprehensive.

Reliability is a concern because Craigslist's parameters are undocumented. They could change at any time, breaking the scraper. Alternatives might include using the city subdomains iteratively, combining both methods, or using official APIs if available (though Craigslist doesn't have a public API, so third-party wrappers like pycraigslist [6] might help).

Examples: If you set search_distance=1000 from a central point, you might get listings from multiple states. But if Craigslist enforces a lower max distance, like 200 miles, it won't work. Testing is needed. Another example: Using postal=92338 (California) in an Ohio subdomain might confuse the results, leading to incorrect geolocation.

Practical implications: Scrapers using this method need to handle parameter changes, validate geolocation accuracy, and possibly combine with subdomain iteration for completeness. Also, rate limiting and IP blocking are risks when scraping at scale.
</think>

The provided Craigslist URL uses geolocation parameters to define a search radius, offering an alternative to traditional city/subdomain-based scraping. Here's a breakdown of its components and implications:

---

### **1. Parameter Analysis**
#### **Core Parameters**
- **`lat=40.4389` / `lon=-83.7035`**:  
  Specifies the geographic center of the search (latitude/longitude coordinates for Rushsylvania, Ohio).
- **`postal=92338`**:  
  A California ZIP code (inconsistent with the Ohio subdomain `limaohio.craigslist.org`). Craigslist likely prioritizes the provided lat/lon over the postal code.
- **`search_distance=1000`**:  
  Defines a **1,000-mile radius** from the geographic center. This is almost certainly in **miles** (Craigslist’s default unit in the U.S.), covering most of the Midwest and East Coast.

#### **Functional Impact**
- **Geographic Scope**: A 1,000-mile radius from central Ohio spans ~12 states, including Pennsylvania, Michigan, and parts of Texas. This bypasses the need to iterate through individual city subdomains (e.g., `cleveland.craigslist.org`, `columbus.craigslist.org`).
- **Dynamic Filtering**: Results are prioritized by proximity to the lat/lon center, not by city.

---

### **2. Benefits vs. Drawbacks**
| **Benefits**                                   | **Drawbacks**                                  |
|------------------------------------------------|------------------------------------------------|
| Avoids subdomain iterations (e.g., 100+ cities)| **Overlap/duplicates**: Listings may reappear as the search radius expands. |
| Centralized data collection                   | **Inconsistent coverage**: Rural areas or low-activity regions may be underrepresented. |
| Simpler code structure                        | **Undocumented limits**: Craigslist might cap `search_distance` (e.g., 200 miles) without warning. |
| Filters non-geotagged listings                 | **Parameter instability**: Craigslist could deprecate/alter parameters. |

---

### **3. Reliability Considerations**
- **Undocumented Limits**: Though `search_distance=1000` works today, Craigslist could enforce a maximum threshold (e.g., 200 miles) or modify how postal codes interact with subdomains.
- **Accuracy Issues**: Mixing a California ZIP (`92338`) with an Ohio subdomain may confuse Craigslist’s backend, leading to inconsistent results.
- **Geotagging Gaps**: Listings without embedded latitude/longitude data might be excluded.

---

### **4. Practical Examples**
- **Scenario 1**: A 1,000-mile radius from Chicago (`lat=41.8781`, `lon=-87.6298`) could scrape listings from Denver to New York in one query.
- **Scenario 2**: Using `postal=94117` (San Francisco) in `newyork.craigslist.org` may return irrelevant or empty results due to geographic mismatch.

---

### **5. Alternatives**
1. **Subdomain Iteration**:  
   Scrape city subdomains individually (e.g., `limaohio.craigslist.org`, `dayton.craigslist.org`).  
   - *Pro*: Guaranteed coverage.  
   - *Con*: Requires managing 100+ URLs and handling regional duplicates.

2. **Hybrid Approach**:  
   Combine large-radius searches with subdomain scraping for gaps.

3. **API Wrappers**:  
   Use libraries like `pycraigslist` [6], which automate parameter handling and pagination.

---

### **Key Takeaways**
- **Efficiency**: Large `search_distance` values reduce the need for subdomain iterations but risk overloading with irrelevant data.  
- **Risk Mitigation**: Monitor for parameter changes and validate results against known geographic boundaries.  
- **Scalability**: For statewide scraping, combine radius-based searches with subdomain checks for comprehensive coverage.  

This method is powerful but hinges on Craigslist’s tolerance for large radii and static parameters. Always design scrapers with fallback mechanisms for parameter deprecation.
<environment_details>
# VSCode Visible Files
frontend/script.js

# VSCode Open Tabs
frontend/index.html
backend/modules/request_handler.py
backend/modules/content_analyzer.py
../../Documents/Cline/MCP/perplexity-mcp/test_mcp.js
.roo/mcp.json
../../.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json
backend/modules/ai_handler.py
backend/config/settings.py
backend/main.py
frontend/script.js
backend/modules/output_generator.py

# Current Time
4/7/2025, 1:01:29 AM (UTC, UTC+0:00)

# Current Mode
PLAN MODE
In this mode you should focus on information gathering, asking questions, and architecting a solution. Once you have a plan, use the plan_mode_respond tool to engage in a conversational back and forth with the user. Do not use the plan_mode_respond tool until you've gathered all the information you need e.g. with read_file or ask_followup_question.
(Remember: If it seems the user wants you to use tools only available in Act Mode, you should ask the user to "toggle to Act mode" (use those words) - they will have to manually do this themselves with the Plan/Act toggle button below. You do not have the ability to switch to Act Mode yourself, and must wait for the user to do it themselves once they are satisfied with the plan. You also cannot present an option to toggle to Act mode, as this will be something you need to direct the user to do manually themselves.)
</environment_details>

---

**Assistant:**

Okay, the analysis confirms your finding: using `lat`, `lon`, and a large `search_distance` is a viable strategy for broad Craigslist searches, potentially covering entire states or regions without iterating through city subdomains. However, it comes with risks like undocumented parameter limits and potential instability.

Based on this, here's a plan to potentially integrate this into your scraper:

1.  **Understand Current Logic:** I need to see how your scraper currently builds URLs and handles locations. I'll start by examining the likely module responsible for requests.
    <read_file>
    <path>backend/modules/request_handler.py</path>
    </read_file>
2.  **Analyze Location Handling:** Depending on what `request_handler.py` reveals, I might

[Response interrupted by a tool use result. Only one tool may be used at a time and should be placed at the end of the message.]

---

**User:**

[read_file for 'backend/modules/request_handler.py'] Result:
# modules/request_handler.py
import requests
import time
import random
import logging # Import logging
# from fake_useragent import UserAgent # Note: fake-useragent might not be needed if Oxylabs handles it (Removed import)

class RequestHandler:
    def __init__(self, config):
        self.config = config
        # self.ua = UserAgent() # Oxylabs handles user agent via 'user_agent_type'
        self.username = config.get('OXYLABS_USERNAME') # Get from config dict
        self.password = config.get('OXYLABS_PASSWORD') # Get from config dict
        self.base_url = 'https://realtime.oxylabs.io/v1/queries'

        if not self.username or not self.password:
             raise ValueError("Oxylabs username and password must be provided in the configuration.")

        # Stats tracking for adaptive behavior
        self.success_count = 0
        self.failure_count = 0
        self.blocked_count = 0 # Tracks failures after all retries

    def get_page(self, url, retry_count=3, backoff_factor=2):
        """Fetch a page using Oxylabs Web Scraper API with intelligent retries"""
        last_error = None
        for attempt in range(retry_count):
            try:
                # Build request payload for Oxylabs API
                payload = {
                    "source": "universal",
                    "url": url,
                    "user_agent_type": "desktop", # Let Oxylabs manage the user agent
                    "render": "html",  # Get rendered HTML with JavaScript processed
                    # Consider adding 'geo_location' if needed for specific cities
                }

                # Add randomized delay before the request
                # Adjust delay based on Oxylabs recommendations if available
                time.sleep(random.uniform(1, 3)) # Reduced delay as Oxylabs manages infrastructure

                logging.info(f"Attempt {attempt + 1}/{retry_count}: Fetching {url} via Oxylabs...")

                # Make request to Oxylabs API
                response = requests.post(
                    self.base_url,
                    auth=(self.username, self.password),
                    json=payload,
                    headers={"Content-Type": "application/json"},
                    timeout=60 # Set a reasonable timeout for the API call
                )

                # Check for successful response from Oxylabs
                if response.status_code == 200:
                    data = response.json()
                    # Oxylabs successful response structure check
                    if ('results' in data and
                        len(data['results']) > 0 and
                        'content' in data['results'][0] and
                        data['results'][0].get('status_code') == 200): # Check status_code within Oxylabs result

                        logging.info(f"Success fetching {url}")
                        self.success_count += 1
                        return data['results'][0]['content'] # Return the HTML content
                    else:
                        # Log Oxylabs specific failure reason if available
                        failure_reason = data.get('_error', 'Unknown Oxylabs issue')
                        status_code_in_result = data['results'][0].get('status_code', 'N/A') if 'results' in data and data['results'] else 'N/A'
                        last_error = f"Oxylabs returned 200 OK but failed for {url}. Reason: {failure_reason}. Internal Status: {status_code_in_result}"
                        logging.warning(last_error) # Use warning level for internal API failures
                        # Treat this as a failure for retry logic
                        self.failure_count += 1

                elif response.status_code == 401:
                     last_error = f"Oxylabs Authentication Failed (401). Check credentials for user '{self.username}'."
                     logging.error(last_error) # Use error level for auth failures
                     # No point retrying on auth failure
                     self.failure_count += (retry_count - attempt) # Count remaining attempts as failures
                     break
                elif response.status_code == 403:
                     last_error = f"Oxylabs Forbidden (403). Check subscription or permissions for {url}."
                     logging.error(last_error) # Use error level for permission issues
                     # May not be worth retrying if it's a permission issue
                     self.failure_count += 1 # Count this attempt as failure
                     # Consider breaking if 403 persists
                elif response.status_code == 429:
                     last_error = f"Oxylabs Rate Limited (429) for {url}."
                     logging.warning(last_error) # Use warning for rate limiting
                     self.failure_count += 1 # Count this attempt as failure
                     # Exponential backoff will handle waiting
                else:
                    # General HTTP error from Oxylabs endpoint
                    last_error = f"Oxylabs API request failed for {url}. Status: {response.status_code}, Body: {response.text[:200]}"
                    logging.warning(last_error) # Use warning for general API errors
                    self.failure_count += 1

                # Implement exponential backoff if request failed (and not auth error)
                if response.status_code != 401:
                    wait_time = backoff_factor ** attempt + random.uniform(0, 1) # Add jitter
                    logging.info(f"Waiting {wait_time:.2f} seconds before retry...")
                    time.sleep(wait_time)

            except requests.exceptions.Timeout:
                last_error = f"Request timed out after 60s for {url} on attempt {attempt + 1}"
                logging.warning(last_error)
                self.failure_count += 1
                # Apply backoff before retrying
                wait_time = backoff_factor ** attempt + random.uniform(0, 1)
                logging.info(f"Waiting {wait_time:.2f} seconds before retry...")
                time.sleep(wait_time)
            except requests.exceptions.RequestException as e:
                last_error = f"Network exception during request for {url} on attempt {attempt + 1}: {str(e)}"
                logging.warning(last_error, exc_info=True) # Log exception info
                self.failure_count += 1
                # Apply backoff before retrying
                wait_time = backoff_factor ** attempt + random.uniform(0, 1)
                logging.info(f"Waiting {wait_time:.2f} seconds before retry...")
                time.sleep(wait_time)
            except Exception as e:
                # Catch unexpected errors during the process
                last_error = f"Unexpected exception during request for {url} on attempt {attempt + 1}: {str(e)}"
                logging.error(last_error, exc_info=True) # Log unexpected errors as ERROR
                self.failure_count += 1
                # Apply backoff before retrying
                wait_time = backoff_factor ** attempt + random.uniform(0, 1)
                logging.info(f"Waiting {wait_time:.2f} seconds before retry...")
                time.sleep(wait_time)


        # If we exhausted all retries
        logging.error(f"Failed to fetch {url} after {retry_count} attempts. Last error: {last_error}")
        self.blocked_count += 1 # Increment blocked count after all retries fail
        return None

    def get_stats(self):
        """Returns the current request statistics."""
        total_attempts = self.success_count + self.failure_count
        success_rate = self.success_count / total_attempts if total_attempts > 0 else 0
        return {
            "success_count": self.success_count,
            "failure_count": self.failure_count, # Failures during retry attempts
            "blocked_count": self.blocked_count, # URLs that failed after all retries
            "total_attempts": total_attempts,
            "success_rate": success_rate
        }

    def is_healthy(self, threshold=0.8):
        """Check if the request handler is working properly based on success rate."""
        stats = self.get_stats()
        # Consider recent performance if stats become large
        return stats["success_rate"] >= threshold if stats["total_attempts"] > 10 else True # Assume healthy if few attempts

    def get_raw_page_for_debug(self, url):
        """Fetches a page using Oxylabs and returns raw content for debugging, minimal retries."""
        logging.debug(f"Attempting to fetch raw HTML for {url}") # Use debug level
        try:
            payload = {
                "source": "universal",
                "url": url,
                "user_agent_type": "desktop",
                "render": "html",
            }
            response = requests.post(
                self.base_url,
                auth=(self.username, self.password),
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=60
            )
            if response.status_code == 200:
                data = response.json()
                if ('results' in data and
                    len(data['results']) > 0 and
                    'content' in data['results'][0] and
                    data['results'][0].get('status_code') == 200):
                    logging.debug(f"Successfully fetched raw HTML.")
                    return data['results'][0]['content']
                else:
                    logging.warning(f"DEBUG: Oxylabs returned 200 OK but failed internally. Response: {data}")
                    return None
            else:
                logging.warning(f"DEBUG: Oxylabs API request failed. Status: {response.status_code}, Body: {response.text[:200]}")
                return None
        except Exception as e:
            logging.error(f"DEBUG: Exception during raw fetch: {str(e)}", exc_info=True)
            return None

<environment_details>
# VSCode Visible Files
frontend/script.js

# VSCode Open Tabs
frontend/index.html
backend/modules/request_handler.py
backend/modules/content_analyzer.py
../../Documents/Cline/MCP/perplexity-mcp/test_mcp.js
.roo/mcp.json
../../.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json
backend/modules/ai_handler.py
backend/config/settings.py
backend/main.py
frontend/script.js
backend/modules/output_generator.py

# Current Time
4/7/2025, 1:01:38 AM (UTC, UTC+0:00)

# Current Mode
PLAN MODE
In this mode you should focus on information gathering, asking questions, and architecting a solution. Once you have a plan, use the plan_mode_respond tool to engage in a conversational back and forth with the user. Do not use the plan_mode_respond tool until you've gathered all the information you need e.g. with read_file or ask_followup_question.
(Remember: If it seems the user wants you to use tools only available in Act Mode, you should ask the user to "toggle to Act mode" (use those words) - they will have to manually do this themselves with the Plan/Act toggle button below. You do not have the ability to switch to Act Mode yourself, and must wait for the user to do it themselves once they are satisfied with the plan. You also cannot present an option to toggle to Act mode, as this will be something you need to direct the user to do manually themselves.)
</environment_details>

---

**Assistant:**

Okay, I've reviewed `backend/modules/request_handler.py`.

This module is responsible for *fetching* web pages using the Oxylabs API. It takes a complete URL as input and handles the request, retries, and potential errors, but it **does not** contain the logic for *constructing* the Craigslist search URLs or managing locations (cities, states, coordinates).

Therefore, the logic for deciding *which* URLs to scrape must reside elsewhere. Likely candidates are:

*   `backend/main.py`: Often the main script orchestrates the process.
*   `backend/modules/city_manager.py`: Name suggests it handles city data.
*   `backend/modules/state_manager.py`: Name suggests it handles state data.

To figure out how to integrate the new large-radius search strategy, we need to find where the current location/URL generation logic lives. Let's examine `backend/main.py` first, as it's the most probable place for the main scraping loop.

```tool_code
<read_file>
<path>backend/main.py</path>
</read_file>
```

[Response interrupted by a tool use result. Only one tool may be used at a time and should be placed at the end of the message.]

---

**User:**

[read_file for 'backend/main.py'] Result:
# main.py (in backend/)
import time
import sys
import os
import random
import logging
import multiprocessing
import shutil
import subprocess
import argparse
import concurrent.futures # Added for ThreadPoolExecutor
from datetime import datetime
from urllib.parse import urljoin

# --- Project Root Definition ---
project_root = os.path.dirname(os.path.abspath(__file__))
repo_root = os.path.dirname(project_root)

# --- Configure Logging ---
log_file = os.path.join(project_root, 'run.log')
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(processName)s - %(levelname)s - %(module)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, mode='w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logging.info("Logging configured.")

logging.info("Attempting module imports...")
try:
    # Import PRE_FILTER_CONFIG as well
    from config.settings import OXYLABS_CONFIG, SEARCH_CONFIG, STATE_CONFIG, AI_CONFIG, PRE_FILTER_CONFIG
    from modules.city_manager import CityManager
    from modules.request_handler import RequestHandler
    from modules.content_analyzer import ContentAnalyzer
    from modules.state_manager import StateManager
    from modules.output_generator import OutputGenerator
    from modules.ai_handler import AIHandler
    logging.info("Module imports successful.")
except ImportError as e:
    logging.critical(f"Error importing modules: {e}", exc_info=True)
    logging.critical("Ensure you are running this script from the 'Gemini-Scraper/backend' directory")
    logging.critical("or that the necessary modules/config are in the correct subdirectories.")
    sys.exit(1)

# --- Lead Processing Helper Function (for ThreadPoolExecutor) ---
def process_single_lead(basic_lead_info, worker_request_handler, worker_content_analyzer, worker_ai_handler, worker_name):
    """Fetches details grades and prepares a single lead dictionary."""
    lead_url = basic_lead_info.get('url')
    if not lead_url:
        logging.warning(f"[{worker_name}] Skipping potential lead: Missing URL.")
        return None, None # Indicate failure/skip

    # Use different log prefix for threaded operations
    log_prefix = f"[{worker_name}-Thread]"

    # --- Add City Domain Check ---
    # Extract city code from the worker_args passed to scrape_city_worker
    # This requires modifying scrape_city_worker to pass city_code down or accessing it differently.
    # Let's modify process_single_lead signature for simplicity here.
    # We need city_code in this function scope. Let's pass it in worker_args and extract it.
    # RETHINK: Modifying scrape_city_worker args is better. Let's assume city_code is passed down.
    # We'll modify the call signature later. For now, assume city_code is available.
    # --- This approach is complex. Let's check the URL domain directly ---
    from urllib.parse import urlparse
    parsed_url = urlparse(lead_url)
    expected_domain = f"{basic_lead_info.get('city_code', 'unknown')}.craigslist.org" # Assume city_code is added to basic_lead_info
    if parsed_url.netloc != expected_domain:
        logging.warning(f"{log_prefix} Skipping lead from different city: {lead_url} (Expected domain: {expected_domain})")
        return None, None # Indicate skip

    logging.info(f"{log_prefix} Fetching lead details: {basic_lead_info.get('title', 'Untitled')} ({lead_url})")
    lead_page_html = worker_request_handler.get_page(lead_url)

    if not lead_page_html:
        logging.warning(f"{log_prefix} Failed to fetch lead detail page: {lead_url}. Skipping lead.")
        return None, None # Indicate failure/skip

    full_lead_details = worker_content_analyzer.analyze_lead_details(lead_page_html, basic_lead_info)

    # AI Grading Step
    ai_grade_result = None
    if worker_ai_handler and worker_ai_handler.client:
        logging.debug(f"{log_prefix} Sending lead to AI for grading: {lead_url}")
        ai_grade_result = worker_ai_handler.grade_lead(
            full_lead_details.get('title', ''),
            full_lead_details.get('description', '')
        )
        full_lead_details['ai_grade'] = ai_grade_result
    else:
        full_lead_details['ai_grade'] = {"is_junk": False, "profitability_score": None, "reasoning": "AI disabled"}

    # Return processed details (including grade) and the grade result separately for filtering logic
    return full_lead_details, ai_grade_result

# --- Worker Function for Parallel Processing (Refactored for Threading) ---
def scrape_city_worker(worker_args):
    """
    Worker function to scrape categories (with pagination) for a single city.
    Uses ThreadPoolExecutor to process leads found on each page in parallel.
    """
    # Unpack args (added limit_leads_per_page)
    city_info, config, limit_pages, limit_categories, limit_leads_per_page, num_threads = worker_args
    city_code = city_info.get('code')
    city_name = city_info.get('name', city_code)
    worker_name = multiprocessing.current_process().name
    logging.info(f"[{worker_name}] Starting worker for city: {city_name} ({city_code})")

    worker_state_manager = None
    worker_ai_handler = None
    city_leads_found_total = 0
    try:
        worker_state_manager = StateManager(config)
        worker_request_handler = RequestHandler(config['OXYLABS_CONFIG'])
        worker_content_analyzer = ContentAnalyzer(config)
        worker_ai_handler = AIHandler(config)

        if not city_code:
            logging.warning(f"[{worker_name}] Skipping city: Missing 'code' in city_info {city_info}")
            return (city_code, 0)

        # Determine categories to process
        all_categories = config['SEARCH_CONFIG'].get('CATEGORIES', [])
        categories_to_process = all_categories[:limit_categories] if limit_categories is not None else all_categories
        if limit_categories is not None:
             logging.warning(f"[{worker_name}] Limiting to first {len(categories_to_process)} categories based on --limit-categories argument.")

        for category_code in categories_to_process:
            current_page = 1
            next_page_url = f"https://{city_code}.craigslist.org/search/{category_code}"
            processed_category_leads = 0
            max_pages_to_scrape = limit_pages # Use the passed limit

            while next_page_url and current_page <= max_pages_to_scrape:
                logging.info(f"[{worker_name}] Fetching category: {category_code} (Page {current_page}/{max_pages_to_scrape}) -> {next_page_url}")
                search_page_html = worker_request_handler.get_page(next_page_url)
                if not search_page_html:
                    logging.warning(f"[{worker_name}] Failed to fetch search page {current_page} for {city_code}/{category_code}. Skipping rest of category.")
                    break

                logging.info(f"[{worker_name}] Parsing search results page {current_page} for {city_code}/{category_code}...")
                potential_leads_count_page = 0
                processed_leads_count_page = 0
                # Pass limit_leads_per_page to the parser
                leads_on_page_basic = worker_content_analyzer.parse_search_page(search_page_html, city_code, limit_leads_per_page)

                # Add city_code to each basic lead info dict for domain checking later
                leads_on_page = []
                for lead in leads_on_page_basic:
                    lead['city_code'] = city_code
                    leads_on_page.append(lead)


                # --- Pre-filter leads based on title/blacklist before AI pre-filter ---
                pre_filtered_leads_basic = []
                blacklist = config.get('PRE_FILTER_CONFIG', {}).get('BLACKLIST_TERMS', [])
                if leads_on_page:
                    potential_leads_count_page = len(leads_on_page)
                    logging.info(f"[{worker_name}] Basic Pre-filtering {potential_leads_count_page} leads from page...")
                    for lead_info in leads_on_page:
                        title = lead_info.get('title', '').lower()
                        # Basic blacklist check
                        if any(term in title for term in blacklist):
                            logging.info(f"[{worker_name}] Basic pre-filtered out (blacklist term): {lead_info.get('url')}")
                            continue # Skip this lead entirely
                        pre_filtered_leads_basic.append(lead_info)
                    logging.info(f"[{worker_name}] {len(pre_filtered_leads_basic)} leads passed basic pre-filtering.")
                else:
                    pre_filtered_leads_basic = []

                # --- AI Pre-filter remaining leads ---
                pre_filtered_leads_ai = []
                if pre_filtered_leads_basic:
                    logging.info(f"[{worker_name}] AI Pre-filtering {len(pre_filtered_leads_basic)} leads...")
                    for lead_info in pre_filtered_leads_basic:
                        title = lead_info.get('title', '')
                        # Using None for snippet as parse_search_page doesn't extract it yet
                        is_potentially_relevant = worker_ai_handler.pre_filter_lead(title, None)
                        if is_potentially_relevant:
                            pre_filtered_leads_ai.append(lead_info)
                        else:
                            logging.info(f"[{worker_name}] AI pre-filtered out (junk/unrelated): {lead_info.get('url')}")
                    logging.info(f"[{worker_name}] {len(pre_filtered_leads_ai)} leads passed AI pre-filtering.")
                else:
                     pre_filtered_leads_ai = []


                # --- Process AI-pre-filtered leads on this page in parallel using threads ---
                if pre_filtered_leads_ai:
                    logging.info(f"[{worker_name}] Processing {len(pre_filtered_leads_ai)} AI pre-filtered leads using {num_threads} threads...")
                    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
                        future_to_lead = {
                            executor.submit(process_single_lead, lead_info, worker_request_handler, worker_content_analyzer, worker_ai_handler, worker_name): lead_info
                            for lead_info in pre_filtered_leads_ai # Use pre_filtered_leads_ai here
                        }
                        for future in concurrent.futures.as_completed(future_to_lead):
                            original_lead_info = future_to_lead[future]
                            try:
                                full_lead_details, ai_grade_result = future.result()
                                if full_lead_details is None: continue # Skip if processing failed

                                # Add the category code to the details before saving
                                full_lead_details['category'] = category_code

                                # Add lead to DB (still done sequentially by main worker thread after thread finishes)
                                if ai_grade_result is not None and not ai_grade_result.get('is_junk', True):
                                    if worker_state_manager.add_lead(full_lead_details):
                                        city_leads_found_total += 1
                                        processed_leads_count_page += 1
                                        score = ai_grade_result.get('profitability_score', 'N/A')
                                        logging.info(f"[{worker_name}] Added graded lead (Score: {score}): {full_lead_details.get('url')}")
                                elif ai_grade_result and ai_grade_result.get('is_junk'):
                                    score = ai_grade_result.get('profitability_score', 'N/A')
                                    reason = ai_grade_result.get('reasoning', 'No reason provided')
                                    logging.info(f"[{worker_name}] Skipping junk lead (AI Filter Score: {score}, Reason: {reason}): {full_lead_details.get('url')}")
                                elif ai_grade_result is None and worker_ai_handler and worker_ai_handler.client:
                                     logging.error(f"[{worker_name}] Skipping lead due to AI grading failure: {full_lead_details.get('url')}")
                                elif not worker_ai_handler or not worker_ai_handler.client:
                                     # Ensure category is added even if AI is disabled
                                     full_lead_details['category'] = category_code
                                     if worker_state_manager.add_lead(full_lead_details):
                                        city_leads_found_total += 1
                                        processed_leads_count_page += 1
                                        logging.info(f"[{worker_name}] Added lead (AI Disabled): {full_lead_details.get('url')}")
                            except Exception as exc:
                                lead_url = original_lead_info.get('url', 'unknown URL')
                                logging.error(f'[{worker_name}] Lead {lead_url} generated an exception during threaded processing: {exc}', exc_info=True)
                # --- End of parallel processing block ---

                logging.info(f"[{worker_name}] Page {current_page}: Parsed {potential_leads_count_page} potential leads, added {processed_leads_count_page} new graded leads for {city_code}/{category_code}.")
                processed_category_leads += processed_leads_count_page

                # Find next page link (Placeholder)
                next_page_relative = None
                if current_page < max_pages_to_scrape: pass # Keep next_page_relative as None

                if next_page_relative and current_page < max_pages_to_scrape:
                    next_page_url = urljoin(next_page_url, next_page_relative)
                    current_page += 1
                    time.sleep(random.uniform(1.0, 2.5))
                else:
                    if current_page >= max_pages_to_scrape: logging.info(f"[{worker_name}] Reached page limit ({max_pages_to_scrape}) for {city_code}/{category_code}.")
                    elif not next_page_relative and current_page < max_pages_to_scrape: logging.info(f"[{worker_name}] No next page link found for {city_code}/{category_code}.")
                    next_page_url = None # Stop pagination

            logging.info(f"[{worker_name}] Finished category {category_code} for {city_code}. Added {processed_category_leads} leads.")
            time.sleep(random.uniform(0.5, 1.5))

        logging.info(f"[{worker_name}] Finished processing {city_name}. Found {city_leads_found_total} new relevant leads in total.")
        return (city_code, city_leads_found_total)

    except Exception as e:
        logging.error(f"[{worker_name}] Error processing city {city_code}: {e}", exc_info=True)
        return (city_code, 0)
    finally:
        if worker_state_manager and worker_state_manager.conn:
            worker_state_manager.close_db()
            logging.info(f"[{worker_name}] Closed DB connection for city {city_code}.")

# --- GitHub Deployment Function ---
def deploy_to_github():
    """Adds all changes, commits, and pushes to the GitHub remote."""
    # (Deployment logic remains the same - uses environment variable)
    github_pat = os.environ.get('GITHUB_PAT')
    if not github_pat:
        logging.error("GITHUB_PAT environment variable not set. Skipping deployment.")
        return
    github_username = "Brandonrz369"
    repo_name = "Gemini-Scraper"
    logging.info(f"Attempting to commit and push changes to GitHub repo: {github_username}/{repo_name}")
    try:
        logging.info(f"Running Git commands in: {repo_root}")
        status_result = subprocess.run(['git', 'status', '--porcelain'], check=True, cwd=repo_root, capture_output=True, text=True)
        if not status_result.stdout and "graded_leads.json" not in status_result.stdout: # Check if only JSON changed potentially
             # Double check if graded_leads.json exists and was modified recently
             json_path = os.path.join(repo_root, 'frontend', 'public', 'graded_leads.json')
             if not os.path.exists(json_path):
                 logging.info("No changes detected and no JSON file found to commit.")
                 return
             # If only JSON changed git status might be empty if .gitignore ignores it, force add
             logging.info("Staging potential JSON update...")
             subprocess.run(['git', 'add', os.path.join('frontend', 'public', 'graded_leads.json')], check=True, cwd=repo_root, capture_output=True, text=True)
             # Check status again
             status_result = subprocess.run(['git', 'status', '--porcelain'], check=True, cwd=repo_root, capture_output=True, text=True)
             if not status_result.stdout:
                 logging.info("No changes detected to commit after attempting to stage JSON.")
                 return

        logging.info("Staging all changes...")
        subprocess.run(['git', 'add', '.'], check=True, cwd=repo_root, capture_output=True, text=True)
        commit_message = f"Update leads dashboard - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        logging.info(f"Committing changes with message: '{commit_message}'")
        subprocess.run(['git', 'commit', '-m', commit_message], check=True, cwd=repo_root, capture_output=True, text=True)
        logging.info("Pushing changes to remote 'origin'...")
        push_env = os.environ.copy()
        push_env['GITHUB_PAT'] = github_pat
        try:
            subprocess.run(['git', 'push', 'origin'], check=True, cwd=repo_root, capture_output=True, text=True, env=push_env)
            logging.info("Changes pushed to GitHub successfully via origin.")
        except subprocess.CalledProcessError as e_push:
             logging.warning(f"Push to origin failed (maybe auth issue?): {e_push.stderr}. Trying fallback push with PAT in URL (less secure)...")
             try:
                 authenticated_repo_url = f"https://{github_pat}@github.com/{github_username}/{repo_name}.git"
                 branch_result = subprocess.run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], check=True, cwd=repo_root, capture_output=True, text=True)
                 current_branch = branch_result.stdout.strip()
                 subprocess.run(['git', 'push', authenticated_repo_url, current_branch], check=True, cwd=repo_root, capture_output=True, text=True)
                 logging.info("Changes pushed to GitHub successfully via authenticated URL.")
             except subprocess.CalledProcessError as e_fallback:
                  logging.error(f"Fallback Git push failed! Stderr: {e_fallback.stderr}")
             except Exception as e_fallback_unexpected:
                  logging.error(f"An unexpected error occurred during fallback GitHub push: {e_fallback_unexpected}", exc_info=True)
    except FileNotFoundError: logging.error("Git command not found. Ensure Git is installed and in PATH.")
    except subprocess.CalledProcessError as e: logging.error(f"Git operation failed! Stderr: {e.stderr}")
    except Exception as e: logging.error(f"An unexpected error during GitHub deployment: {e}", exc_info=True)

# --- Main Execution ---
def main(args):
    logging.info(f"--- Craigslist Lead Generation Agent Started (List Size: {args.list_size}) ---")
    os.chdir(project_root)
    logging.info(f"Working directory set to: {os.getcwd()}")
    # Include PRE_FILTER_CONFIG in the main config dict
    config = {
        'OXYLABS_CONFIG': OXYLABS_CONFIG,
        'SEARCH_CONFIG': SEARCH_CONFIG,
        'STATE_CONFIG': STATE_CONFIG,
        'AI_CONFIG': AI_CONFIG,
        'PRE_FILTER_CONFIG': PRE_FILTER_CONFIG
    }
    state_manager = None
    try:
        logging.info("Initializing main manager instances...")
        os.makedirs(os.path.join(project_root, 'data'), exist_ok=True)
        state_manager = StateManager(config)
        city_manager = CityManager(config, state_manager)
        output_generator = OutputGenerator(config)
        logging.info("Main manager instances initialized.")

        logging.info("Testing connection to Oxylabs...")
        # (Oxylabs check remains the same)
        temp_request_handler = RequestHandler(config['OXYLABS_CONFIG'])
        test_url = "https://newyork.craigslist.org/search/web"
        try:
            content = temp_request_handler.get_page(test_url, retry_count=1)
            if content and "craigslist" in content.lower(): logging.info("✅ Oxylabs connection successful.")
            else: logging.error("❌ Oxylabs connection failed during pre-check."); sys.exit(1)
        except Exception as e: logging.critical(f"Oxylabs connection pre-check error: {e}", exc_info=True); sys.exit(1)
        del temp_request_handler

        logging.info("--- System Status & Setup ---")
        cities_to_process_full = city_manager.get_cities_by_population(args.list_size)
        total_cities_in_list = len(cities_to_process_full)
        logging.info(f"Found {total_cities_in_list} cities configured for list size '{args.list_size}'.")

        if args.limit_cities is not None and args.limit_cities > 0:
            cities_to_process = cities_to_process_full[:args.limit_cities]
            logging.warning(f"Limiting run to the first {len(cities_to_process)} cities based on --limit-cities argument.")
        else:
            cities_to_process = cities_to_process_full

        total_cities_to_process = len(cities_to_process)
        logging.info(f"Processing {total_cities_to_process} cities.")
        logging.info(f"Total leads currently in database: {state_manager._get_total_leads_count()}")
        if not cities_to_process: logging.warning("No cities found to process. Exiting."); sys.exit(0)

        # Determine max pages per category based on argument or default (3)
        max_pages = args.limit_pages if args.limit_pages is not None and args.limit_pages > 0 else 3
        logging.info(f"Scraping up to {max_pages} pages per category.")

        logging.info("--- Starting Parallel Scrape ---")
        # Pool size still based on number of cities to process
        pool_size = min((os.cpu_count() or 4) * 2, total_cities_to_process) if total_cities_to_process > 0 else 1
        logging.info(f"Initializing multiprocessing pool with {pool_size} workers.")
        # Pass limits, leads_per_page limit, and thread count to each worker
        worker_args_list = [(city_info, config, max_pages, args.limit_categories, args.limit_leads_per_page, args.num_threads) for city_info in cities_to_process]
        total_leads_found_this_session = 0
        processed_cities_count = 0

        with multiprocessing.Pool(processes=pool_size) as pool:
            logging.info(f"Distributing {len(worker_args_list)} cities/tasks to workers...")
            try:
                for result in pool.imap_unordered(scrape_city_worker, worker_args_list):
                    processed_cities_count += 1
                    city_code, leads_found = result
                    if leads_found > 0: total_leads_found_this_session += leads_found
                    logging.info(f"Worker finished for {city_code}. Found {leads_found} leads. ({processed_cities_count}/{total_cities_to_process} cities complete)")
                logging.info("All workers finished.")
            except KeyboardInterrupt: logging.warning("KeyboardInterrupt received. Terminating pool..."); pool.terminate(); pool.join(); logging.warning("Pool terminated."); sys.exit(1)
            except Exception as e: logging.error(f"Error during pool execution: {e}", exc_info=True); pool.terminate(); pool.join(); sys.exit(1)

        logging.info("--- Scrape Finished ---")
        state_manager._set_progress_value("last_full_run_completed", datetime.now().isoformat())
        logging.info(f"Processed {processed_cities_count} cities.")
